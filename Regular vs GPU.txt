void vector_add_cpu(float *a, float *b, float *c, int N) {
    for (int i = 0; i < N; i++) {
        c[i] = a[i] + b[i];  // sequential or at best CPU multi-threaded
    }
}
vs
__global__ void vector_add_gpu(float *a, float *b, float *c, int N) {
    int idx = threadIdx.x + blockIdx.x * blockDim.x;  // unique thread index
    if (idx < N) {
        c[idx] = a[idx] + b[idx];  // each thread handles one element
    }
}
------------------------------------------------------------------------------------------
void matrix_mul_cpu(float* A, float* B, float* C, int M, int N, int K) {
    for (int row = 0; row < M; row++) {
        for (int col = 0; col < K; col++) {
            float sum = 0;
            for (int i = 0; i < N; i++) {
                sum += A[row*N + i] * B[i*K + col];
            }
            C[row*K + col] = sum;
        }
    }
}
vs
__global__ void matrix_mul_gpu(float* A, float* B, float* C, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < M && col < K) {
        float sum = 0;
        for (int i = 0; i < N; i++) {
            sum += A[row*N + i] * B[i*K + col];
        }
        C[row*K + col] = sum;
    }
}
----------------------------------------------------------------------------
void prefix_sum_cpu(int *in, int *out, int N) {
    out[0] = in[0];
    for (int i = 1; i < N; i++) {
        out[i] = out[i-1] + in[i];  // strictly sequential
    }
}

vs
__global__ void prefix_sum_gpu(int *in, int *out, int N) {
    __shared__ int temp[1024];  // shared memory (fast, per block)
    int tid = threadIdx.x;
    int i = blockIdx.x * blockDim.x + tid;

    if (i < N) temp[tid] = in[i];
    __syncthreads();

    // parallel prefix sum (up-sweep)
    for (int stride = 1; stride < blockDim.x; stride *= 2) {
        int val = 0;
        if (tid >= stride) val = temp[tid - stride];
        __syncthreads();
        temp[tid] += val;
        __syncthreads();
    }

    if (i < N) out[i] = temp[tid];
}
-------------------------------------------------------------------------
